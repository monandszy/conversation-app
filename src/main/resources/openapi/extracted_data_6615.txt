models/chat-bison-001
models/text-bison-001
models/embedding-gecko-001
models/gemini-1.0-pro-latest
models/gemini-1.0-pro
models/gemini-pro
models/gemini-1.0-pro-001
models/gemini-1.0-pro-vision-latest
models/gemini-pro-vision
models/gemini-1.5-pro-latest
models/gemini-1.5-pro-001
models/gemini-1.5-pro-002
models/gemini-1.5-pro
models/gemini-1.5-pro-exp-0801
models/gemini-1.5-pro-exp-0827
models/gemini-1.5-flash-latest
models/gemini-1.5-flash-001
models/gemini-1.5-flash-001-tuning
models/gemini-1.5-flash
models/gemini-1.5-flash-exp-0827
models/gemini-1.5-flash-002
models/gemini-1.5-flash-8b
models/gemini-1.5-flash-8b-001
models/gemini-1.5-flash-8b-latest
models/gemini-1.5-flash-8b-exp-0827
models/gemini-1.5-flash-8b-exp-0924
models/embedding-001
models/text-embedding-004
models/aqa

class Model {
    baseModelId: null
    description: A legacy text-only model optimized for chat conversations
    displayName: PaLM 2 Chat (Legacy)
    inputTokenLimit: 4096
    maxTemperature: null
    name: models/chat-bison-001
    outputTokenLimit: 1024
    supportedGenerationMethods: [generateMessage, countMessageTokens]
    temperature: 0.25
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: A legacy model that understands text and generates text as an output
    displayName: PaLM 2 (Legacy)
    inputTokenLimit: 8196
    maxTemperature: null
    name: models/text-bison-001
    outputTokenLimit: 1024
    supportedGenerationMethods: [generateText, countTextTokens, createTunedTextModel]
    temperature: 0.7
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Obtain a distributed representation of a text.
    displayName: Embedding Gecko
    inputTokenLimit: 1024
    maxTemperature: null
    name: models/embedding-gecko-001
    outputTokenLimit: 1
    supportedGenerationMethods: [embedText, countTextTokens]
    temperature: null
    topK: null
    topP: null
    version: 001
}
class Model {
    baseModelId: null
    description: The best model for scaling across a wide range of tasks. This is the latest model.
    displayName: Gemini 1.0 Pro Latest
    inputTokenLimit: 30720
    maxTemperature: null
    name: models/gemini-1.0-pro-latest
    outputTokenLimit: 2048
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 0.9
    topK: null
    topP: 1.0
    version: 001
}
class Model {
    baseModelId: null
    description: The best model for scaling across a wide range of tasks
    displayName: Gemini 1.0 Pro
    inputTokenLimit: 30720
    maxTemperature: null
    name: models/gemini-1.0-pro
    outputTokenLimit: 2048
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 0.9
    topK: null
    topP: 1.0
    version: 001
}
class Model {
    baseModelId: null
    description: The best model for scaling across a wide range of tasks
    displayName: Gemini 1.0 Pro
    inputTokenLimit: 30720
    maxTemperature: null
    name: models/gemini-pro
    outputTokenLimit: 2048
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 0.9
    topK: null
    topP: 1.0
    version: 001
}
class Model {
    baseModelId: null
    description: The best model for scaling across a wide range of tasks. This is a stable model that supports tuning.
    displayName: Gemini 1.0 Pro 001 (Tuning)
    inputTokenLimit: 30720
    maxTemperature: null
    name: models/gemini-1.0-pro-001
    outputTokenLimit: 2048
    supportedGenerationMethods: [generateContent, countTokens, createTunedModel]
    temperature: 0.9
    topK: null
    topP: 1.0
    version: 001
}
class Model {
    baseModelId: null
    description: The best image understanding model to handle a broad range of applications
    displayName: Gemini 1.0 Pro Vision
    inputTokenLimit: 12288
    maxTemperature: null
    name: models/gemini-1.0-pro-vision-latest
    outputTokenLimit: 4096
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 0.4
    topK: 32
    topP: 1.0
    version: 001
}
class Model {
    baseModelId: null
    description: The best image understanding model to handle a broad range of applications
    displayName: Gemini 1.0 Pro Vision
    inputTokenLimit: 12288
    maxTemperature: null
    name: models/gemini-pro-vision
    outputTokenLimit: 4096
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 0.4
    topK: 32
    topP: 1.0
    version: 001
}
class Model {
    baseModelId: null
    description: Mid-size multimodal model that supports up to 2 million tokens
    displayName: Gemini 1.5 Pro Latest
    inputTokenLimit: 2000000
    maxTemperature: 2.0
    name: models/gemini-1.5-pro-latest
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Mid-size multimodal model that supports up to 2 million tokens
    displayName: Gemini 1.5 Pro 001
    inputTokenLimit: 2000000
    maxTemperature: 2.0
    name: models/gemini-1.5-pro-001
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens, createCachedContent]
    temperature: 1.0
    topK: 64
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Mid-size multimodal model that supports up to 2 million tokens
    displayName: Gemini 1.5 Pro 002
    inputTokenLimit: 2000000
    maxTemperature: 2.0
    name: models/gemini-1.5-pro-002
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens, createCachedContent]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 002
}
class Model {
    baseModelId: null
    description: Mid-size multimodal model that supports up to 2 million tokens
    displayName: Gemini 1.5 Pro
    inputTokenLimit: 2000000
    maxTemperature: 2.0
    name: models/gemini-1.5-pro
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Mid-size multimodal model that supports up to 2 million tokens
    displayName: Gemini 1.5 Pro Experimental 0801
    inputTokenLimit: 2000000
    maxTemperature: 2.0
    name: models/gemini-1.5-pro-exp-0801
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 64
    topP: 0.95
    version: exp-0801
}
class Model {
    baseModelId: null
    description: Mid-size multimodal model that supports up to 2 million tokens
    displayName: Gemini 1.5 Pro Experimental 0827
    inputTokenLimit: 2000000
    maxTemperature: 2.0
    name: models/gemini-1.5-pro-exp-0827
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 64
    topP: 0.95
    version: exp-0827
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash Latest
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-latest
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash 001
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-001
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens, createCachedContent]
    temperature: 1.0
    topK: 64
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash 001 Tuning
    inputTokenLimit: 16384
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-001-tuning
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens, createTunedModel]
    temperature: 1.0
    topK: 64
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash Experimental 0827
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-exp-0827
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 64
    topP: 0.95
    version: exp-0827
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash 002
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-002
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens, createCachedContent]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 002
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash-8B
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-8b
    outputTokenLimit: 8192
    supportedGenerationMethods: [createCachedContent, generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash-8B 001
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-8b-001
    outputTokenLimit: 8192
    supportedGenerationMethods: [createCachedContent, generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash-8B Latest
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-8b-latest
    outputTokenLimit: 8192
    supportedGenerationMethods: [createCachedContent, generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash 8B Experimental 0827
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-8b-exp-0827
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Fast and versatile multimodal model for scaling across diverse tasks
    displayName: Gemini 1.5 Flash 8B Experimental 0924
    inputTokenLimit: 1000000
    maxTemperature: 2.0
    name: models/gemini-1.5-flash-8b-exp-0924
    outputTokenLimit: 8192
    supportedGenerationMethods: [generateContent, countTokens]
    temperature: 1.0
    topK: 40
    topP: 0.95
    version: 001
}
class Model {
    baseModelId: null
    description: Obtain a distributed representation of a text.
    displayName: Embedding 001
    inputTokenLimit: 2048
    maxTemperature: null
    name: models/embedding-001
    outputTokenLimit: 1
    supportedGenerationMethods: [embedContent]
    temperature: null
    topK: null
    topP: null
    version: 001
}
class Model {
    baseModelId: null
    description: Obtain a distributed representation of a text.
    displayName: Text Embedding 004
    inputTokenLimit: 2048
    maxTemperature: null
    name: models/text-embedding-004
    outputTokenLimit: 1
    supportedGenerationMethods: [embedContent]
    temperature: null
    topK: null
    topP: null
    version: 004
}
class Model {
    baseModelId: null
    description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.
    displayName: Model that performs Attributed Question Answering.
    inputTokenLimit: 7168
    maxTemperature: null
    name: models/aqa
    outputTokenLimit: 1024
    supportedGenerationMethods: [generateAnswer]
    temperature: 0.2
    topK: 40
    topP: 1.0
    version: 001
}